{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 - Knowledge Graph Construction - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np0plMPXRvoq"
   },
   "source": [
    "In this lesson, you'll continue with knowledge graph construction. The previous lesson created the domain\n",
    "graph from CSV files according to the construction plan. Now, you will process the markdown files,\n",
    "chunking them up into the lexical graph and the subject graph which will connect to the domain graph\n",
    "for a complete knowledge graph. \n",
    "\n",
    "You will learn:\n",
    "- how to use Neo4j's graphrag library to perform the chunking and entity extraction\n",
    "- techniques for entity resolution\n",
    "  \n",
    "\n",
    "<img src=\"images/last.png\" width=\"600\">\n",
    "\n",
    "**Note**: This notebook uses Cypher queries to build the domain graph from CSV files. Don't worry if you're unfamiliar with Cypher ‚Äî focus on understanding the big picture of how the unstructured data is transformed into a graph structure based on the extraction plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>To access the helper.py, neo4j_for_adk.py and tools.py files :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two tools, with helper functions:\n",
    "  1. `make_kg_builder` - to chunk markdown and produce the lexical and subject graphs\n",
    "  2. `correlate_subject_and_domain_nodes` - to connect the subject graph to the domain graph\n",
    "- Input: `approved_files`, `approved_construction_plan`, `approved_entities`, `approved_fact_types`\n",
    "- Output: a completed knowledge graph with domain, lexical and subject graphs\n",
    "  \n",
    "**Workflow**\n",
    "\n",
    "1. The context is initialized with an `approved_construction_plan` and `approved_files`\n",
    "2. For each markdown file, `make_kg_builder` is called to create a construction pipeline\n",
    "3. For each resulting entity label, `correlate_subject_and_domain_nodes` will connect the subject and domain graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual import of needed libraries, loading of environment variables, and connection to Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Common Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 319,
    "id": "sbwxKypOSBkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 181,
    "id": "MI_qvZJrSJuR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-CImNyf7iYyRsd8iM8Llkd7fyzyCFJ', created=1758591934, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_cbf1785567', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Yes, I'm ready. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=27, total_tokens=40, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "\n",
      "OpenAI ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT_4O)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'query_result': [{'message': 'Neo4j is Ready!'}]}\n"
     ]
    }
   ],
   "source": [
    "# Check connection to Neo4j by sending a query\n",
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Load Part of the Domain Graph with a Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're only loading the product nodes from the domain graph, because they're the only nodes that you'll use to connect the domain graph to the lexical graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'query_result': [{'nonEntityLabels': ['Product']},\n",
       "  {'nonEntityLabels': ['Assembly']},\n",
       "  {'nonEntityLabels': ['Part']},\n",
       "  {'nonEntityLabels': ['Supplier']},\n",
       "  {'nonEntityLabels': ['__KGBuilder__', 'Chunk']},\n",
       "  {'nonEntityLabels': ['__KGBuilder__', 'Document']}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import load_product_nodes\n",
    "\n",
    "load_product_nodes()\n",
    "\n",
    "# expect to find non-entity nodes with a \"Product\" label\n",
    "graphdb.send_query(\"MATCH (n) WHERE NOT n:`__Entity__` return DISTINCT labels(n) as nonEntityLabels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 Initialize State from Previous Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 1099
   },
   "outputs": [],
   "source": [
    "# the approved construction plan should look something like this...\n",
    "approved_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"Contains\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"Is_Part_Of\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"Supplied_By\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "approved_files = [\n",
    "    \"product_reviews/gothenburg_table_reviews.md\",\n",
    "    \"product_reviews/helsingborg_dresser_reviews.md\",\n",
    "    \"product_reviews/jonkoping_coffee_table_reviews.md\",\n",
    "    \"product_reviews/linkoping_bed_reviews.md\",\n",
    "    \"product_reviews/malmo_desk_reviews.md\",\n",
    "    \"product_reviews/norrkoping_nightstand_reviews.md\",\n",
    "    \"product_reviews/orebro_lamp_reviews.md\",\n",
    "    \"product_reviews/stockholm_chair_reviews.md\",\n",
    "    \"product_reviews/uppsala_sofa_reviews.md\",\n",
    "    \"product_reviews/vasteras_bookshelf_reviews.md\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# approved entities from the `ner_agent` of Lesson 7\n",
    "approved_entities = ['Product', 'Issue', 'Feature', 'Location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "# approved fact types from the `relevant_fact_agent` of Lesson 7\n",
    "approved_fact_types = {'has_issue': {'subject_label': 'Product', 'predicate_label': 'has_issue', 'object_label': 'Issue'}, 'includes_feature': {'subject_label': 'Product', 'predicate_label': 'includes_feature', 'object_label': 'Feature'}, 'used_in_location': {'subject_label': 'Product', 'predicate_label': 'used_in_location', 'object_label': 'Location'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Tool Definitions for loading, chunking and entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neo4j GraphRAG library has a convenient `SimpleKGPipeline` which you can use to process chunks and extract entities with relationships.\n",
    "\n",
    "For the markdown files you will be processing, you'll need to create some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 SimpleKGPipeline Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/KG_pipeline.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    " \n",
    "# for example, creating a KG pipeline requires these arguments\n",
    "if False:\n",
    "    example = SimpleKGPipeline(\n",
    "        llm=None, # the LLM to use for Entity and Relation extraction\n",
    "        driver=None,  # a neo4j driver to write results to graph\n",
    "        embedder=None,  # an Embedder for chunks\n",
    "        from_pdf=True,   # sortof True because you will use a custom loader\n",
    "        pdf_loader=None, # the custom loader for Markdown\n",
    "        text_splitter=None, # the splitter you defined above\n",
    "        schema=None, # that you just defined above\n",
    "        prompt_template=None, # the template used for entity extraction on each chunk\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 Text-Splitter for Chunking up the Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custom text splitter that uses regex patterns to chunk markdown text. This splitter breaks documents at specified delimiters (like \"---\") to create meaningful text segments for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 421
   },
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "\n",
    "# Define a custom text splitter. Chunking strategy could be yet-another-agent\n",
    "class RegexTextSplitter(TextSplitter):\n",
    "    \"\"\"Split text using regex matched delimiters.\"\"\"\n",
    "    def __init__(self, re: str):\n",
    "        self.re = re\n",
    "    \n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Splits a piece of text into chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split.\n",
    "\n",
    "        Returns:\n",
    "            TextChunks: A list of chunks.\n",
    "        \"\"\"\n",
    "        texts = re.split(self.re, text)\n",
    "        i = 0\n",
    "        chunks = [TextChunk(text=str(text), index=i) for (i, text) in enumerate(texts)]\n",
    "        return TextChunks(chunks=chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 Custom Markdown Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom loader adapts the Neo4j GraphRAG PDF loader to work with markdown files. It reads markdown content, extracts the document title from the first H1 header, and wraps it in the expected document format for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "# custom file data loader\n",
    "\n",
    "from neo4j_graphrag.experimental.components.pdf_loader import DataLoader\n",
    "from neo4j_graphrag.experimental.components.types import PdfDocument, DocumentInfo\n",
    "\n",
    "class MarkdownDataLoader(DataLoader):\n",
    "    def extract_title(self,markdown_text):\n",
    "        # Define a regex pattern to match the first h1 header\n",
    "        pattern = r'^# (.+)$'\n",
    "\n",
    "        # Search for the first match in the markdown text\n",
    "        match = re.search(pattern, markdown_text, re.MULTILINE)\n",
    "\n",
    "        # Return the matched group if found\n",
    "        return match.group(1) if match else \"Untitled\"\n",
    "\n",
    "    async def run(self, filepath: Path, metadata = {}) -> PdfDocument:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            markdown_text = f.read()\n",
    "        doc_headline = self.extract_title(markdown_text)\n",
    "        markdown_info = DocumentInfo(\n",
    "            path=str(filepath),\n",
    "            metadata={\n",
    "                \"title\": doc_headline,\n",
    "            }\n",
    "        )\n",
    "        return PdfDocument(text=markdown_text, document_info=markdown_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.4 Set up LLM, Embedder and Neo4j Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the core components needed for the Neo4j GraphRAG pipeline: an OpenAI LLM for entity extraction, an embeddings model for vectorizing text chunks, and the Neo4j database driver for graph storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# create an OpenAI client for use by Neo4j GraphRAG\n",
    "llm_for_neo4j = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "\n",
    "# use OpenAI for creating embeddings\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# use the same driver set up by neo4j_for_adk.py\n",
    "neo4j_driver = graphdb.get_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.5 Entity Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the approved entity types from the previous workflow as the allowed node types for entity extraction. This constrains the LLM to only extract entities of these specific types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_node_types:  ['Product', 'Issue', 'Feature', 'Location']\n"
     ]
    }
   ],
   "source": [
    "# approved entities list can be used directly \n",
    "schema_node_types = approved_entities\n",
    "\n",
    "print(\"schema_node_types: \", schema_node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the approved fact types into relationship types by extracting the predicate labels and converting them to uppercase format for the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_relationship_types:  ['HAS_ISSUE', 'INCLUDES_FEATURE', 'USED_IN_LOCATION']\n"
     ]
    }
   ],
   "source": [
    "# the keys from approved fact types dictionary can be used for relationship types\n",
    "schema_relationship_types = [key.upper() for key in approved_fact_types.keys()]\n",
    "\n",
    "print(\"schema_relationship_types: \", schema_relationship_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create relationship patterns by converting fact types into tuples that specify allowed relationships between specific node types (subject-predicate-object patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_patterns: [['Product', 'HAS_ISSUE', 'Issue'], ['Product', 'INCLUDES_FEATURE', 'Feature'], ['Product', 'USED_IN_LOCATION', 'Location']]\n"
     ]
    }
   ],
   "source": [
    "# rewrite the fact types into a list of tuples\n",
    "schema_patterns = [\n",
    "    [ fact['subject_label'], fact['predicate_label'].upper(), fact['object_label'] ]\n",
    "    for fact in approved_fact_types.values()\n",
    "]\n",
    "\n",
    "print(\"schema_patterns:\", schema_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the complete entity schema dictionary that will guide the LLM's entity extraction, combining node types, relationship types, and patterns into a single configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the complete entity schema\n",
    "entity_schema = {\n",
    "    \"node_types\": schema_node_types,\n",
    "    \"relationship_types\": schema_relationship_types,\n",
    "    \"patterns\": schema_patterns,\n",
    "    \"additional_node_types\": False, # True would be less strict, allowing unknown node types\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.6 Contexualized Entity Extraction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function extracts the first few lines from a file to provide context for entity extraction. This context helps the LLM better understand the document structure and content when processing individual chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_context(file_path:str, num_lines=5) -> str:\n",
    "    \"\"\"Helper function to extract the first few lines of a file\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        num_lines (int, optional): Number of lines to extract. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        str: First few lines of the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a contextualized prompt template for entity and relationship extraction. It combines general extraction instructions with file-specific context to improve the accuracy of the LLM's entity recognition on each text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-chunk entity extraction prompt, with context\n",
    "def contextualize_er_extraction_prompt(context:str) -> str:\n",
    "    \"\"\"Creates a prompt with pre-amble file content for context during entity+relationship extraction.\n",
    "    The context is concatenated into the string, which later will be used as a template\n",
    "    for values like {schema} and {text}.\n",
    "    \"\"\"\n",
    "    general_instructions = \"\"\"\n",
    "    You are a top-tier algorithm designed for extracting\n",
    "    information in structured formats to build a knowledge graph.\n",
    "\n",
    "    Extract the entities (nodes) and specify their type from the following text.\n",
    "    Also extract the relationships between these nodes.\n",
    "\n",
    "    Return result as JSON using the following format:\n",
    "    {{\"nodes\": [ {{\"id\": \"0\", \"label\": \"Person\", \"properties\": {{\"name\": \"John\"}} }}],\n",
    "    \"relationships\": [{{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"since\": \"2024-08-01\"}} }}] }}\n",
    "\n",
    "    Use only the following node and relationship types (if provided):\n",
    "    {schema}\n",
    "\n",
    "    Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
    "    Do respect the source and target node types for relationship and\n",
    "    the relationship direction.\n",
    "\n",
    "    Make sure you adhere to the following rules to produce valid JSON objects:\n",
    "    - Do not return any additional information other than the JSON in it.\n",
    "    - Omit any backticks around the JSON - simply output the JSON on its own.\n",
    "    - The JSON object must not wrapped into a list - it is its own JSON object.\n",
    "    - Property names must be enclosed in double quotes\n",
    "    \"\"\"\n",
    "\n",
    "    context_goes_here = f\"\"\"\n",
    "    Consider the following context to help identify entities and relationships:\n",
    "    <context>\n",
    "    {context}  \n",
    "    </context>\"\"\"\n",
    "    \n",
    "    input_goes_here = \"\"\"\n",
    "    Input text:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    return general_instructions + \"\\n\" + context_goes_here + \"\\n\" + input_goes_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Make and Use the Knowledge Graph (KG) builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Make the Neo4j KG Builder Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a customized KG builder pipeline for a specific file by extracting file context and creating a contextualized extraction prompt. It combines all the previously defined components (loader, splitter, schema, LLM) into a complete pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each approved markdown file by creating a KG builder pipeline and running it asynchronously. This extracts entities and relationships from the text chunks and stores them in the Neo4j database as the subject graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kg_builder(file_path:str) -> SimpleKGPipeline:\n",
    "    \"\"\"Builds a KG builder for a given file, which is used to contextualize the chunking and entity extraction.\"\"\"\n",
    "    context = file_context(file_path)\n",
    "    contextualized_prompt = contextualize_er_extraction_prompt(context)\n",
    "\n",
    "    return SimpleKGPipeline(\n",
    "        llm=llm_for_neo4j, # the LLM to use for Entity and Relation extraction\n",
    "        driver=neo4j_driver,  # a neo4j driver to write results to graph\n",
    "        embedder=embedder,  # an Embedder for chunks\n",
    "        from_pdf=True,   # sortof True because you will use a custom loader\n",
    "        pdf_loader=MarkdownDataLoader(), # the custom loader for Markdown\n",
    "        text_splitter=RegexTextSplitter(\"---\"), # the splitter you defined above\n",
    "        schema=entity_schema, # that you just defined above\n",
    "        prompt_template=contextualized_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> üö®\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by LLMs can vary with each execution due to their stochastic nature. Your results might differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: product_reviews/gothenburg_table_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 141, 'number_of_created_nodes': 47}}\n",
      "Processing file: product_reviews/helsingborg_dresser_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 162, 'number_of_created_nodes': 47}}\n",
      "Processing file: product_reviews/jonkoping_coffee_table_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 173, 'number_of_created_nodes': 50}}\n",
      "Processing file: product_reviews/linkoping_bed_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 187, 'number_of_created_nodes': 50}}\n",
      "Processing file: product_reviews/malmo_desk_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 197, 'number_of_created_nodes': 52}}\n",
      "Processing file: product_reviews/norrkoping_nightstand_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 204, 'number_of_created_nodes': 52}}\n",
      "Processing file: product_reviews/orebro_lamp_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 226, 'number_of_created_nodes': 64}}\n",
      "Processing file: product_reviews/stockholm_chair_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 229, 'number_of_created_nodes': 72}}\n",
      "Processing file: product_reviews/uppsala_sofa_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 243, 'number_of_created_nodes': 79}}\n",
      "Processing file: product_reviews/vasteras_bookshelf_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 259, 'number_of_created_nodes': 86}}\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "# CAUTION: Original function is not idempotent \n",
    "from helper import get_neo4j_import_dir\n",
    "\n",
    "neo4j_import_dir = get_neo4j_import_dir() or \".\"\n",
    "\n",
    "for file_name in approved_files:\n",
    "    file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    kg_builder = make_kg_builder(file_path)\n",
    "    results = await kg_builder.run_async(file_path=str(file_path))\n",
    "    print(\"\\tResults:\", results.result)\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking which files need processing...\n",
      "Already processed (10 files):\n",
      "  ‚úì product_reviews/gothenburg_table_reviews.md\n",
      "  ‚úì product_reviews/helsingborg_dresser_reviews.md\n",
      "  ‚úì product_reviews/jonkoping_coffee_table_reviews.md\n",
      "  ‚úì product_reviews/linkoping_bed_reviews.md\n",
      "  ‚úì product_reviews/malmo_desk_reviews.md\n",
      "  ‚úì product_reviews/norrkoping_nightstand_reviews.md\n",
      "  ‚úì product_reviews/orebro_lamp_reviews.md\n",
      "  ‚úì product_reviews/stockholm_chair_reviews.md\n",
      "  ‚úì product_reviews/uppsala_sofa_reviews.md\n",
      "  ‚úì product_reviews/vasteras_bookshelf_reviews.md\n",
      "\n",
      "Files to process (0 files):\n",
      "\n",
      "No new files to process - all files have already been processed!\n"
     ]
    }
   ],
   "source": [
    "# Function below IS idempotent\n",
    "# Modified version that checks for already processed files to avoid duplicates\n",
    "from helper import get_neo4j_import_dir\n",
    "\n",
    "def get_processed_files():\n",
    "    \"\"\"Get list of files that have already been processed (have Document nodes)\"\"\"\n",
    "    results = graphdb.send_query(\"\"\"\n",
    "    MATCH (d:Document)\n",
    "    RETURN DISTINCT d.path AS file_path\n",
    "    \"\"\")\n",
    "    processed_paths = [row['file_path'] for row in results['query_result'] if row['file_path']]\n",
    "    return processed_paths\n",
    "\n",
    "def is_file_processed(file_path):\n",
    "    \"\"\"Check if a file has already been processed\"\"\"\n",
    "    processed_files = get_processed_files()\n",
    "    return any(file_path in processed_path for processed_path in processed_files)\n",
    "\n",
    "neo4j_import_dir = get_neo4j_import_dir() or \".\"\n",
    "\n",
    "print(\"Checking which files need processing...\")\n",
    "files_to_process = []\n",
    "already_processed = []\n",
    "\n",
    "for file_name in approved_files:\n",
    "    file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "    if is_file_processed(file_path):\n",
    "        already_processed.append(file_name)\n",
    "    else:\n",
    "        files_to_process.append(file_name)\n",
    "\n",
    "print(f\"Already processed ({len(already_processed)} files):\")\n",
    "for file in already_processed:\n",
    "    print(f\"  ‚úì {file}\")\n",
    "\n",
    "print(f\"\\nFiles to process ({len(files_to_process)} files):\")\n",
    "for file in files_to_process:\n",
    "    print(f\"  ‚Üí {file}\")\n",
    "\n",
    "# Only process files that haven't been processed yet\n",
    "if files_to_process:\n",
    "    print(f\"\\nProcessing {len(files_to_process)} new files...\")\n",
    "    for file_name in files_to_process:\n",
    "        file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        kg_builder = make_kg_builder(file_path)\n",
    "        results = await kg_builder.run_async(file_path=str(file_path))\n",
    "        print(\"\\tResults:\", results.result)\n",
    "    print(\"All new files processed.\")\n",
    "else:\n",
    "    print(\"\\nNo new files to process - all files have already been processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 10 documents, 70 chunks, 165 entities\n",
      "‚úÖ Database state looks good!\n"
     ]
    }
   ],
   "source": [
    "# Check current database state - we need chunks and entities, not just documents\n",
    "current_state = graphdb.send_query(\"\"\"\n",
    "MATCH (d:Document) WITH count(d) AS docs\n",
    "OPTIONAL MATCH (c:Chunk) WITH docs, count(c) AS chunks  \n",
    "OPTIONAL MATCH (e:__Entity__) WITH docs, chunks, count(e) AS entities\n",
    "RETURN docs, chunks, entities\n",
    "\"\"\")\n",
    "\n",
    "if current_state['query_result']:\n",
    "    state = current_state['query_result'][0]\n",
    "    print(f\"Current state: {state['docs']} documents, {state['chunks']} chunks, {state['entities']} entities\")\n",
    "    \n",
    "    # If we have documents but no chunks/entities, we need to reprocess\n",
    "    if state['docs'] > 0 and (state['chunks'] == 0 or state['entities'] == 0):\n",
    "        print(\"\\n‚ö†Ô∏è  Documents exist but chunks/entities are missing - need to reprocess!\")\n",
    "        \n",
    "        # Option 1: Remove document nodes and reprocess everything\n",
    "        print(\"\\nOption 1: Remove all documents and reprocess from scratch\")\n",
    "        \n",
    "        # Option 2: Force reprocess despite existing documents\n",
    "        print(\"Option 2: Force reprocess despite existing documents\")\n",
    "        \n",
    "        print(\"\\nLet's go with Option 2 - force reprocess to rebuild chunks and entities...\")\n",
    "        \n",
    "        for file_name in approved_files:\n",
    "            file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "            print(f\"Force processing file: {file_name}\")\n",
    "            kg_builder = make_kg_builder(file_path)\n",
    "            results = await kg_builder.run_async(file_path=str(file_path))\n",
    "            print(f\"\\tResults: {results.result}\")\n",
    "        \n",
    "        print(\"All files reprocessed!\")\n",
    "        \n",
    "        # Check final state\n",
    "        final_state = graphdb.send_query(\"\"\"\n",
    "        MATCH (d:Document) WITH count(d) AS docs\n",
    "        OPTIONAL MATCH (c:Chunk) WITH docs, count(c) AS chunks  \n",
    "        OPTIONAL MATCH (e:__Entity__) WITH docs, chunks, count(e) AS entities\n",
    "        RETURN docs, chunks, entities\n",
    "        \"\"\")\n",
    "        \n",
    "        if final_state['query_result']:\n",
    "            final = final_state['query_result'][0]\n",
    "            print(f\"\\nFinal state: {final['docs']} documents, {final['chunks']} chunks, {final['entities']} entities\")\n",
    "    else:\n",
    "        print(\"‚úÖ Database state looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Reprocessing Complete!\n",
      "‚úÖ Final state: 10 documents, 70 chunks, 165 entities\n",
      "\n",
      "üìä Extracted entity types:\n",
      "  - No entity types found or query error\n",
      "\n",
      "‚úÖ No duplicate documents found - clean processing successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify the reprocessing results\n",
    "verification = graphdb.send_query(\"\"\"\n",
    "MATCH (d:Document) WITH count(d) AS docs\n",
    "OPTIONAL MATCH (c:Chunk) WITH docs, count(c) AS chunks  \n",
    "OPTIONAL MATCH (e:__Entity__) WITH docs, chunks, count(e) AS entities\n",
    "RETURN docs, chunks, entities\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéâ Reprocessing Complete!\")\n",
    "if verification['query_result']:\n",
    "    state = verification['query_result'][0]\n",
    "    print(f\"‚úÖ Final state: {state['docs']} documents, {state['chunks']} chunks, {state['entities']} entities\")\n",
    "\n",
    "# Check entity types that were extracted\n",
    "entity_types = graphdb.send_query(\"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "WITH DISTINCT labels(e) AS entity_labels\n",
    "UNWIND entity_labels AS entity_label\n",
    "WHERE NOT entity_label STARTS WITH \"__\"\n",
    "RETURN entity_label, count(*) AS count\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Extracted entity types:\")\n",
    "if entity_types and 'query_result' in entity_types:\n",
    "    for row in entity_types['query_result']:\n",
    "        print(f\"  - {row['entity_label']}: {row['count']} entities\")\n",
    "else:\n",
    "    print(\"  - No entity types found or query error\")\n",
    "\n",
    "# Check for any duplicate documents (should be none)\n",
    "dup_check = graphdb.send_query(\"\"\"\n",
    "MATCH (d:Document)\n",
    "WITH d.path AS file_path, collect(d) AS documents\n",
    "WHERE size(documents) > 1\n",
    "RETURN file_path, size(documents) AS duplicate_count\n",
    "\"\"\")\n",
    "\n",
    "if dup_check and 'query_result' in dup_check and dup_check['query_result']:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(dup_check['query_result'])} files with duplicates:\")\n",
    "    for row in dup_check['query_result']:\n",
    "        print(f\"  - {row['file_path']}: {row['duplicate_count']} copies\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicate documents found - clean processing successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting completely fresh processing...\n",
      "‚úÖ Cleared 10 processing nodes\n",
      "\n",
      "üìÅ Processing all files from scratch...\n",
      "Processing: product_reviews/gothenburg_table_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 20, 'number_of_created_nodes': 7}}\n",
      "Processing: product_reviews/helsingborg_dresser_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 50, 'number_of_created_nodes': 10}}\n",
      "Processing: product_reviews/jonkoping_coffee_table_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 64, 'number_of_created_nodes': 13}}\n",
      "Processing: product_reviews/linkoping_bed_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 77, 'number_of_created_nodes': 17}}\n",
      "Processing: product_reviews/malmo_desk_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 93, 'number_of_created_nodes': 26}}\n",
      "Processing: product_reviews/norrkoping_nightstand_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 112, 'number_of_created_nodes': 32}}\n",
      "Processing: product_reviews/orebro_lamp_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 138, 'number_of_created_nodes': 48}}\n",
      "Processing: product_reviews/stockholm_chair_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 148, 'number_of_created_nodes': 55}}\n",
      "Processing: product_reviews/uppsala_sofa_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 161, 'number_of_created_nodes': 62}}\n",
      "Processing: product_reviews/vasteras_bookshelf_reviews.md\n",
      "  ‚úÖ Results: {'resolver': {'number_of_nodes_to_resolve': 180, 'number_of_created_nodes': 72}}\n",
      "\n",
      "üéâ Fresh processing complete!\n",
      "\n",
      "üìä Final state: 10 documents, 70 chunks, 165 entities\n",
      "‚úÖ No duplicates - clean processing successful!\n"
     ]
    }
   ],
   "source": [
    "# The issue is that the cleanup function removes chunks when documents are removed\n",
    "# Let's start fresh with a clean approach: remove all documents and reprocess properly\n",
    "\n",
    "print(\"üîÑ Starting completely fresh processing...\")\n",
    "\n",
    "# Step 1: Clear all processing-related nodes to start clean\n",
    "clear_all = graphdb.send_query(\"\"\"\n",
    "MATCH (n)\n",
    "WHERE n:Document OR n:Chunk OR n:`__Entity__`\n",
    "DETACH DELETE n\n",
    "RETURN count(n) AS cleared_nodes\n",
    "\"\"\")\n",
    "\n",
    "cleared = clear_all['query_result'][0]['cleared_nodes'] if clear_all['query_result'] else 0\n",
    "print(f\"‚úÖ Cleared {cleared} processing nodes\")\n",
    "\n",
    "# Step 2: Now process all files fresh (they won't be detected as \"already processed\")\n",
    "print(\"\\nüìÅ Processing all files from scratch...\")\n",
    "\n",
    "for file_name in approved_files:\n",
    "    file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "    print(f\"Processing: {file_name}\")\n",
    "    kg_builder = make_kg_builder(file_path)\n",
    "    results = await kg_builder.run_async(file_path=str(file_path))\n",
    "    if results and hasattr(results, 'result'):\n",
    "        print(f\"  ‚úÖ Results: {results.result}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Completed\")\n",
    "\n",
    "print(\"\\nüéâ Fresh processing complete!\")\n",
    "\n",
    "# Step 3: Verify final state\n",
    "final_state = graphdb.send_query(\"\"\"\n",
    "MATCH (d:Document) WITH count(d) AS docs\n",
    "OPTIONAL MATCH (c:Chunk) WITH docs, count(c) AS chunks  \n",
    "OPTIONAL MATCH (e:__Entity__) WITH docs, chunks, count(e) AS entities\n",
    "RETURN docs, chunks, entities\n",
    "\"\"\")\n",
    "\n",
    "if final_state['query_result']:\n",
    "    state = final_state['query_result'][0]\n",
    "    print(f\"\\nüìä Final state: {state['docs']} documents, {state['chunks']} chunks, {state['entities']} entities\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_final_check = graphdb.send_query(\"\"\"\n",
    "MATCH (d:Document)\n",
    "WITH d.path AS file_path, collect(d) AS documents\n",
    "WHERE size(documents) > 1\n",
    "RETURN count(*) AS duplicate_files\n",
    "\"\"\")\n",
    "\n",
    "duplicates = dup_final_check['query_result'][0]['duplicate_files'] if dup_final_check['query_result'] else 0\n",
    "if duplicates == 0:\n",
    "    print(\"‚úÖ No duplicates - clean processing successful!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {duplicates} files still have duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing extracted entities...\n",
      "\n",
      "üìä Entity types extracted:\n",
      "  ‚úÖ Issue: 79 entities\n",
      "  ‚úÖ Feature: 60 entities\n",
      "  ‚úÖ Location: 16 entities\n",
      "  ‚úÖ Product: 10 entities\n",
      "\n",
      "üîó No relationships found between entities\n",
      "\n",
      "‚úÖ Knowledge graph construction (re)processing complete!\n",
      "üöÄ Ready to proceed with entity resolution to connect subject graph to domain graph!\n"
     ]
    }
   ],
   "source": [
    "# Check what entity types were successfully extracted\n",
    "print(\"üîç Analyzing extracted entities...\")\n",
    "\n",
    "entity_analysis = graphdb.send_query(\"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "UNWIND labels(e) AS label\n",
    "WITH label, count(e) AS entity_count\n",
    "WHERE NOT label STARTS WITH \"__\"\n",
    "RETURN label AS entity_type, entity_count\n",
    "ORDER BY entity_count DESC\n",
    "\"\"\")\n",
    "\n",
    "if entity_analysis and 'query_result' in entity_analysis and entity_analysis['query_result']:\n",
    "    print(\"\\nüìä Entity types extracted:\")\n",
    "    for row in entity_analysis['query_result']:\n",
    "        print(f\"  ‚úÖ {row['entity_type']}: {row['entity_count']} entities\")\n",
    "        \n",
    "    # Check relationships between entities\n",
    "    rel_check = graphdb.send_query(\"\"\"\n",
    "    MATCH (e1:__Entity__)-[r]->(e2:__Entity__)\n",
    "    RETURN type(r) AS relationship_type, count(r) AS count\n",
    "    ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    if rel_check and 'query_result' in rel_check and rel_check['query_result']:\n",
    "        print(\"\\nüîó Relationships between entities:\")\n",
    "        for row in rel_check['query_result']:\n",
    "            print(f\"  ‚û°Ô∏è  {row['relationship_type']}: {row['count']} relationships\")\n",
    "    else:\n",
    "        print(\"\\nüîó No relationships found between entities\")\n",
    "else:\n",
    "    print(\"‚ùå No entities found or query error\")\n",
    "\n",
    "print(\"\\n‚úÖ Knowledge graph construction (re)processing complete!\")\n",
    "print(\"üöÄ Ready to proceed with entity resolution to connect subject graph to domain graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Tool Definition for Entity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Connect entities in the subject graph to entities in the domain graph.\n",
    "\n",
    "For each type of entity in the subject graph, you will devise a strategy for correlating\n",
    "with the right node in the domain graph. \n",
    "\n",
    "For example, you should expect that Products with product names exist in the subject graph,\n",
    "and that these should correlate with products in the domain graph.\n",
    "\n",
    "To do this, you will:\n",
    "1. find the unique entity labels in the subject graph\n",
    "2. find the unique node labels in the domain graph\n",
    "3. attempt to correlate property keys\n",
    "4. perform entity resolution by analyzing the similarity of property values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 Unique Entity Labels in the Subject Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique triples of (subject, predicate, object) will give you an idea about what the subject graph looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the Neo4j database to find all nodes that have the `__Entity__` label (entities created by the knowledge graph builder) and return their distinct label combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_labels': ['__KGBuilder__', '__Entity__', 'Location']},\n",
       " {'entity_labels': ['Product', '__KGBuilder__', '__Entity__']},\n",
       " {'entity_labels': ['__KGBuilder__', '__Entity__', 'Issue']},\n",
       " {'entity_labels': ['__KGBuilder__', '__Entity__', 'Feature']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, take a look at the entity labels\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    RETURN DISTINCT labels(n) AS entity_labels\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the label arrays into individual label strings using UNWIND, which transforms the array of labels into separate rows for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_label': '__KGBuilder__'},\n",
       " {'entity_label': '__Entity__'},\n",
       " {'entity_label': 'Location'},\n",
       " {'entity_label': 'Product'},\n",
       " {'entity_label': 'Issue'},\n",
       " {'entity_label': 'Feature'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unwind those lists of labels\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT labels(n) AS entity_labels\n",
    "    UNWIND entity_labels AS entity_label\n",
    "    RETURN DISTINCT entity_label\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out internal Neo4j labels that start with double underscores (\"__\") to focus only on the meaningful entity type labels extracted from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_label': 'Location'},\n",
       " {'entity_label': 'Product'},\n",
       " {'entity_label': 'Issue'},\n",
       " {'entity_label': 'Feature'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out labels that start with \"__\"\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT labels(n) AS entity_labels\n",
    "    UNWIND entity_labels AS entity_label\n",
    "    WITH entity_label\n",
    "    WHERE NOT entity_label STARTS WITH \"__\"\n",
    "    RETURN entity_label\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the previous query steps into a reusable function that returns all unique entity labels from the subject graph, excluding internal Neo4j system labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the query into a callable function\n",
    "def find_unique_entity_labels():\n",
    "    result = graphdb.send_query(\"\"\"MATCH (n)\n",
    "        WHERE n:`__Entity__`\n",
    "        WITH DISTINCT labels(n) AS entity_labels\n",
    "        UNWIND entity_labels AS entity_label\n",
    "        WITH entity_label\n",
    "        WHERE NOT entity_label STARTS WITH \"__\"\n",
    "        RETURN collect(entity_label) as unique_entity_labels\n",
    "        \"\"\")\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['message'])\n",
    "    return result['query_result'][0]['unique_entity_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function to see what entity labels were actually extracted from the processed markdown files into the subject graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entity labels:  ['Location', 'Product', 'Issue', 'Feature']\n"
     ]
    }
   ],
   "source": [
    "# try out the function\n",
    "unique_entity_labels = find_unique_entity_labels()\n",
    "\n",
    "print(\"Unique entity labels: \", unique_entity_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 Unique Entity Keys for a Given Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to find all unique property keys for entities of a specific label in the subject graph. This helps identify what properties are available for matching with domain graph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['footprint',\n",
       " 'color temperature',\n",
       " 'weighted base',\n",
       " 'LED driver',\n",
       " 'equivalent illuminance',\n",
       " 'shade material',\n",
       " 'power consumption',\n",
       " '__tmp_internal_id',\n",
       " 'name',\n",
       " 'CRI',\n",
       " 'surface_area',\n",
       " 'features',\n",
       " 'issue',\n",
       " 'description',\n",
       " 'rating',\n",
       " 'size',\n",
       " 'design',\n",
       " 'ease_of_cleaning',\n",
       " 'quality',\n",
       " 'ease_of_assembly',\n",
       " 'durability',\n",
       " 'material',\n",
       " 'dimensions',\n",
       " 'finish',\n",
       " 'scratch_resistance',\n",
       " 'load_capacity',\n",
       " 'construction',\n",
       " 'assembly',\n",
       " 'shelf_depth',\n",
       " 'quality_to_price_ratio',\n",
       " 'stability',\n",
       " 'review',\n",
       " 'assembly_people',\n",
       " 'parts_labeling',\n",
       " 'assembly_time',\n",
       " 'assembly_instructions',\n",
       " 'support']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_unique_entity_keys(entityLabel:str):\n",
    "    result = graphdb.send_query(f\"\"\"MATCH (n:`{entityLabel}`)\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT keys(n) as entityKeys\n",
    "    UNWIND entityKeys as entityKey\n",
    "    RETURN collect(distinct(entityKey)) as unique_entity_keys\n",
    "    \"\"\")\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['error_message'])\n",
    "    return result['query_result'][0]['unique_entity_keys']\n",
    "        \n",
    "# try out the function to get the unique keys for \n",
    "# subject nodes labeled as Product\n",
    "find_unique_entity_keys(\"Product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3 Unique Domain keys for a Given Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to find unique property keys for nodes of a specific label in the domain graph (nodes without the `__Entity__` label). This enables comparison with subject graph properties for entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description', 'product_id', 'product_name', 'price']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_unique_domain_keys(domainLabel:str):\n",
    "    result = graphdb.send_query(f\"\"\"MATCH (n:`{domainLabel}`)\n",
    "    WHERE NOT n:`__Entity__` // exclude entities created by the KG builder, these should be domain nodes\n",
    "    WITH DISTINCT keys(n) as domainKeys\n",
    "    UNWIND domainKeys as domainKey\n",
    "    RETURN collect(distinct(domainKey)) as unique_domain_keys\n",
    "    \"\"\")\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['error_message'])\n",
    "    return result['query_result'][0]['unique_domain_keys']\n",
    "        \n",
    "\n",
    "find_unique_domain_keys(\"Product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4 Normalize keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple version of \"stemming\" as done in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to normalize property key names by removing label prefixes, converting to lowercase, and standardizing spacing. This helps match similar property keys that may have different naming conventions between subject and domain graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "name\n",
      "name\n",
      "price\n"
     ]
    }
   ],
   "source": [
    "def normalize_key(label:str, key:str) -> str:\n",
    "    \"\"\"Normalizes a a property key for a given label.\n",
    "\n",
    "    Keys are normalized by:\n",
    "    - lowercase the key\n",
    "    - remove any leading/trailing whitespace\n",
    "    - remove label prefix from key\n",
    "    - replace internal whitespace with \"_\"\n",
    "\n",
    "    for example: \n",
    "        - \"Product_name\" -> \"name\"\n",
    "        - \"product name\" -> \"name\"\n",
    "        - \"price\" -> \"price\n",
    "\n",
    "    Args:\n",
    "        label (str): The label to normalize keys for\n",
    "        keys (List[str]): The list of keys to normalize\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The normalized list of keys\n",
    "    \"\"\"\n",
    "    lowercase_key = key.lower()\n",
    "    unprefixed_key = re.sub(f\"^{label.lower()}[_ ]*\", \"\", lowercase_key)\n",
    "    normalized_key = re.sub(\" \", \"_\", unprefixed_key)\n",
    "    return normalized_key\n",
    "\n",
    "print(normalize_key(\"Product\", \"Product_name\"))\n",
    "print(normalize_key(\"Product\", \"Product Name\"))\n",
    "print(normalize_key(\"Product\", \"product name\"))\n",
    "print(normalize_key(\"Product\", \"price\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5 Correlate Keys for a Given Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fuzzy string matching to find correlations between entity graph property keys and domain graph property keys. This function compares normalized key names and returns matches above a similarity threshold, helping identify which properties can be used for entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product correlated keys (entity key, domain key, similarity score)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('name', 'product_name', 1.0),\n",
       " ('description', 'description', 1.0),\n",
       " ('design', 'description', 0.5882352941176471),\n",
       " ('dimensions', 'description', 0.5714285714285714),\n",
       " ('review', 'price', 0.5454545454545454),\n",
       " ('construction', 'description', 0.5217391304347826)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the rapidfuzz library for fuzzy text similarity scoring\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# for a given label, get pairs of entity and domain keys that correlate\n",
    "def correlate_entity_and_domain_keys(label: str, entity_keys: list[str], domain_keys: list[str], similarity: float = 0.9) -> list[tuple[str, str]]:\n",
    "    correlated_keys = []\n",
    "    for entity_key in entity_keys:\n",
    "        for domain_key in domain_keys:\n",
    "            # only consider exact matches. this could use fuzzy matching\n",
    "            normalized_entity_key = normalize_key(label, entity_key)\n",
    "            normalized_domain_key = normalize_key(label, domain_key)\n",
    "            # rapidfuzz similarity is 0.0 -> 100.0, so divide by 100 for 0.0 -> 1.0\n",
    "            fuzzy_similarity = (fuzz.ratio(normalized_entity_key, normalized_domain_key) / 100)\n",
    "            if (fuzzy_similarity > similarity): \n",
    "                correlated_keys.append((entity_key, domain_key, fuzzy_similarity))\n",
    "    correlated_keys.sort(key=lambda x: x[2], reverse=True)\n",
    "    return correlated_keys\n",
    "\n",
    "label = \"Product\"\n",
    "entity_keys = find_unique_entity_keys(label)\n",
    "domain_keys = find_unique_domain_keys(label)\n",
    "\n",
    "# try correlating with a low-ish threshold\n",
    "correlated_keys = correlate_entity_and_domain_keys(label, entity_keys, domain_keys, similarity=0.5)\n",
    "\n",
    "print(f\"{label} correlated keys (entity key, domain key, similarity score)...\")\n",
    "\n",
    "# show the keys\n",
    "correlated_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6 Value Similarity using Jaro‚ÄìWinkler Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaro‚ÄìWinkler distance is a string comparison method, emphasizing common prefixes to favor strings that match from the start. \n",
    "\n",
    "- measures \"edit distance\" between two strings\n",
    "- produces values from 0.0 (exact match) to 1.0 (no similarity)\n",
    "- use `similarity = 1.0 - distance` to get a similarity score\n",
    "\n",
    "See [Jaro-WinklerDistance](https://en.wikipedia.org/wiki/Jaro‚ÄìWinkler_distance) for details.\n",
    "\n",
    "Ideally, you would sample a few values that you expect to correlate well, trying different similarity metrics\n",
    "to find one that works well for that particular value pair. \n",
    "\n",
    "Neo4j provides many [text similarity functions](https://neo4j-contrib.github.io/neo4j-apoc-procedures/3.4/utilities/text-functions/). \n",
    "Other options include:\n",
    "- [`apoc.text.hammingDistance`]()\n",
    "- [`apoc.text.levenshteinSimilarity`]()\n",
    "- [`apoc.text.sorensenDiceSimilarity`]()\n",
    "- [`apoc.text.fuzzyMatch`]()\n",
    "\n",
    "And for vector similarity:\n",
    "- `vector.similarity.cosine` to directly calculate cosine similarity\n",
    "- `db.index.vector.queryNodes` to perform vector similarity search (after first creating a vector index on the domain nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the entity resolution logic into a reusable function that correlates subject and domain nodes based on property value similarity using Jaro-Winkler distance. This creates the bridge between extracted entities and the existing domain graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entityValue': '√ñrebro Lamp', 'domainValue': '√ñrebro Lamp', 'score': 0.0},\n",
       " {'entityValue': 'Malm√∂ Desk', 'domainValue': 'Malm√∂ Desk', 'score': 0.0},\n",
       " {'entityValue': 'Helsingborg Dresser',\n",
       "  'domainValue': 'Helsingborg Dresser',\n",
       "  'score': 0.0}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the Jaro-Winkler function to calculate distance between product names\n",
    "results = graphdb.send_query(f\"\"\"\n",
    "// MATCH all pairs of subject and domain nodes -- this is an expensive cartesian product\n",
    "MATCH (entity:`Product`:`__Entity__`), (domain:`Product`)\n",
    "WITH entity, domain, apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) as score\n",
    "// experiment with different thresholds to see how the results change\n",
    "WHERE score < 0.4\n",
    "RETURN entity[$entityKey] AS entityValue, domain[$domainKey] AS domainValue, score\n",
    "// experiment with different limits to see more or fewer pairs\n",
    "LIMIT 3\n",
    "\"\"\", {\n",
    "    \"entityKey\": \"name\",\n",
    "    \"domainKey\": \"product_name\"\n",
    "})\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `CORRESPONDS_TO` relationships between subject graph entities and domain graph nodes with similar property values. Uses MERGE to avoid duplicate relationships and adds timestamps to track when correlations were established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:184',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:5'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:191',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:2'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:212',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:7'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:269',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:0'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:292',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:8'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:327',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:3'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:357',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:9'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:404',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:1'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:413',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:4'},\n",
       " {'entity_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:448',\n",
       "  'r': ({}, 'CORRESPONDS_TO', {}),\n",
       "  'domain_id': '4:c777dfdf-a9a9-44d4-876a-2f0342280b26:6'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect all corresponding nodes with a relationship\n",
    "results = graphdb.send_query(f\"\"\"\n",
    "MATCH (entity:`Product`:`__Entity__`),(domain:`Product`)\n",
    "// use the score as a predicate to filter the pairs. this is better\n",
    "WHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < 0.1\n",
    "MERGE (entity)-[r:CORRESPONDS_TO]->(domain)\n",
    "ON CREATE SET r.created_at = datetime()\n",
    "ON MATCH SET r.updated_at = datetime()\n",
    "RETURN elementId(entity) as entity_id, r, elementId(domain) as domain_id\n",
    "\"\"\", {\n",
    "    \"entityKey\": \"name\",\n",
    "    \"domainKey\": \"product_name\"\n",
    "})\n",
    "\n",
    "results['query_result']\n",
    "\n",
    "# run this repeatedly to illustrate that MERGE only happens once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Jaro-Winkler distance function by finding all pairs of subject and domain Product nodes where the name properties have similarity scores below a threshold, showing potential matches for entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entityLabel': 'Product', 'relationshipCount': 10}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap as a function\n",
    "def correlate_subject_and_domain_nodes(label: str, entity_key: str, domain_key: str, similarity: float = 0.9) -> dict:\n",
    "    \"\"\"Correlate entity and domain nodes based on label, entity key, and domain key,\n",
    "    where the corresponding values of the entity and domain properties are similar\n",
    "    \n",
    "    For example, if you have a label \"Person\" and an entity key \"name\", and a domain key \"person_name\",\n",
    "    this function will create a relationship like:\n",
    "    (:Person:`__Entity__` {name: \"John\"})-[:CORRELATES_TO]->(:Person {person_name: \"John\"}) \n",
    "    \n",
    "\n",
    "    Args:\n",
    "        label (str): The label of the entity and domain nodes.\n",
    "        entity_key (str): The key of the entity node.\n",
    "        domain_key (str): The key of the domain node.\n",
    "        similarity (float, optional): The similarity threshold for correlation. Defaults to 0.9.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the correlation between the entity and domain nodes.\n",
    "    \"\"\"\n",
    "    results = graphdb.send_query(f\"\"\"\n",
    "    MATCH (entity:`{label}`:`__Entity__`),(domain:`{label}`)\n",
    "    WHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < $distance\n",
    "    MERGE (entity)-[r:CORRESPONDS_TO]->(domain)\n",
    "    ON CREATE SET r.created_at = datetime() // MERGE sub-clause when the relationship is newly created\n",
    "    ON MATCH SET r.updated_at = datetime()  // MERGE sub-clause when the relationship already exists\n",
    "    RETURN $entityLabel as entityLabel, count(r) as relationshipCount\n",
    "    \"\"\", {\n",
    "        \"entityLabel\": label,\n",
    "        \"entityKey\": entity_key,\n",
    "        \"domainKey\": domain_key,\n",
    "        \"distance\": (1.0 - similarity)\n",
    "    })\n",
    "\n",
    "    if results['status'] == 'error':\n",
    "        raise Exception(results['error_message'])\n",
    "\n",
    "    return results['query_result']\n",
    "\n",
    "\n",
    "correlate_subject_and_domain_nodes(\"Product\", \"name\", \"product_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7 Correlate and connect the subject nodes to the domain nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the complete entity resolution workflow by iterating through all extracted entity labels, finding the best property key correlations, and automatically creating connections between the subject graph and domain graph to complete the knowledge graph integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlating entities labeled Location...\n",
      "\tNo correlation found\n",
      "Correlating entities labeled Product...\n",
      "\tbased on: ('name', 'product_name', 1.0)\n",
      "Correlating entities labeled Issue...\n",
      "\tNo correlation found\n",
      "Correlating entities labeled Feature...\n",
      "\tNo correlation found\n",
      "\tNo correlation found\n",
      "Correlating entities labeled Feature...\n",
      "\tNo correlation found\n"
     ]
    }
   ],
   "source": [
    "# do it all:\n",
    "# - loop over all entity labels\n",
    "# - correlate the keys\n",
    "# - correlate (and connect) the nodes\n",
    "for entity_label in find_unique_entity_labels():\n",
    "    print(f\"Correlating entities labeled {entity_label}...\")\n",
    "    \n",
    "    entity_keys = find_unique_entity_keys(entity_label)\n",
    "    domain_keys = find_unique_domain_keys(entity_label)\n",
    "\n",
    "    correlated_keys = correlate_entity_and_domain_keys(entity_label, entity_keys, domain_keys, similarity=0.8)\n",
    "\n",
    "    if (len(correlated_keys) > 0):\n",
    "        top_correlated_keypair = correlated_keys[0]\n",
    "        print(\"\\tbased on:\", top_correlated_keypair)\n",
    "        correlate_subject_and_domain_nodes(entity_label, top_correlated_keypair[0], top_correlated_keypair[1])\n",
    "    else:\n",
    "        print(\"\\tNo correlation found\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "adk-neo4j (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
